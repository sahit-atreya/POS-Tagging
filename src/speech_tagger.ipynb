{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# IMPORT STATEMENTS\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "from sklearn.model_selection import KFold\r\n",
    "from sklearn.naive_bayes import GaussianNB\r\n",
    "from sklearn.tree import DecisionTreeClassifier\r\n",
    "from sklearn.metrics import precision_recall_fscore_support\r\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\r\n",
    "from sklearn import preprocessing\r\n",
    "le = preprocessing.LabelEncoder()\r\n",
    "\r\n",
    "# if the function is zipped, can use the below libraries to unzip them\r\n",
    "import gzip\r\n",
    "import shutil\r\n",
    "import xml.etree.ElementTree as XET\r\n",
    "\r\n",
    "# these files are from PolEval2017 and contains data fro the task (supervised learning models)\r\n",
    "training_file = '..\\\\data\\\\training.xml'\r\n",
    "validation_file = '..\\\\data\\\\validate.xml'\r\n",
    "testing_file = '..\\\\data\\\\test.xml'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# feature_extraction_from_xml(data_file, orth_or_lemma, ctag_or_full)\r\n",
    "This function is used to extract the orth/lemma and the ctag/full-ctag from the data_file based on the specifications in the second and third arguments\r\n",
    "Returns all the orths/lemmas, ctags/full_ctags, and the number of tokens per chunk (sentence).\r\n",
    "\r\n",
    "Must pass 'orth' or 'lemma' for argument 2\r\n",
    "Must pass 'ctag' or 'full' for argument 3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def feature_extraction(data_file, orth_or_lemma, ctag_or_full):\r\n",
    "    # if (orth_or_lemma != 'orth') or (orth_or_lemma != 'lemma'):\r\n",
    "    #     print(\"Pass the right parameters to orth_or_lemma\")\r\n",
    "    #     return\r\n",
    "    # if (ctag_or_full != 'ctag') or (ctag_or_full != 'full'):\r\n",
    "    #     print(\"Pass the right parameters to ctag_or_full\")\r\n",
    "    #     return\r\n",
    "\r\n",
    "\r\n",
    "    orths = []\r\n",
    "    ctags = []\r\n",
    "    tokens_in_chunk = []\r\n",
    "\r\n",
    "    tree = XET.parse(data_file)\r\n",
    "    root = tree.getroot()\r\n",
    "\r\n",
    "    # highest level tag in the xml (one chunk is one sentence)\r\n",
    "    for chunk in root.findall('chunk'):\r\n",
    "        curr_chunk = 0\r\n",
    "\r\n",
    "        orths.extend([\"_\", \"_\", \"_\"])\r\n",
    "        ctags.extend([\"NONE\", \"NONE\", \"NONE\"])\r\n",
    "        for token in chunk.findall('tok'):\r\n",
    "            if orth_or_lemma == 'orth':\r\n",
    "                orth = token.find('orth').text\r\n",
    "            elif orth_or_lemma == 'lemma':\r\n",
    "                orth = token.find('lex').find('base').text\r\n",
    "\r\n",
    "            if ctag_or_full == 'ctag':\r\n",
    "                ctag = token.find('lex').find('ctag').text.split(':', 1)[0]\r\n",
    "            elif ctag_or_full == 'full':\r\n",
    "                ctag = token.find('lex').find('ctag').text\r\n",
    "            curr_chunk += 1\r\n",
    "\r\n",
    "            orths.append(orth)\r\n",
    "            ctags.append(ctag)\r\n",
    "        orths.extend([\"_\", \"_\", \"_\"])\r\n",
    "        ctags.extend([\"NONE\", \"NONE\", \"NONE\"])\r\n",
    "        tokens_in_chunk.append(curr_chunk)\r\n",
    "    return orths, ctags, tokens_in_chunk"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def generate_dataframe(orths_or_lemmas, ctags_or_fulls, in_chunks):\r\n",
    "    a1, a2, a3, a4, a5, a6, a7, tags = [], [], [], [], [], [], [], []\r\n",
    "    curr_orth = 3 # keeps track of which orth we are currently in\r\n",
    "\r\n",
    "    # total number of orths/lemmas to iterate through\r\n",
    "    orths_count = len(orths_or_lemmas)\r\n",
    "\r\n",
    "    for i in range(len(in_chunks)):\r\n",
    "        for j in range(curr_orth, (curr_orth + in_chunks[i])):\r\n",
    "            a1.append(orths_or_lemmas[j-3])\r\n",
    "            a2.append(orths_or_lemmas[j-2])\r\n",
    "            a3.append(orths_or_lemmas[j-1])\r\n",
    "            a4.append(orths_or_lemmas[j])\r\n",
    "            a5.append(orths_or_lemmas[j+1])\r\n",
    "            a6.append(orths_or_lemmas[j+2])\r\n",
    "            a7.append(orths_or_lemmas[j+3])\r\n",
    "            tags.append(ctags_or_fulls[j])\r\n",
    "            curr_orth += 1\r\n",
    "        curr_orth += 6\r\n",
    "\r\n",
    "    total_tag_size = len(tags)\r\n",
    "    df = pd.DataFrame(list(zip(a1, a2, a3, a4, a5, a6, a7, tags)), columns=['a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7', 'class'])\r\n",
    "\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Uses sklearns preprocessor for label encoding\r\n",
    "df is for the dataframe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def labels_and_forFitting(df):\r\n",
    "    labels = np.asarray(df['class'].astype(\"category\").cat.codes.tolist())\r\n",
    "    values = df.drop(columns=['class']).values # represents just the features\r\n",
    "    le.fit(values.ravel()) # assigns numeric codes for unique values in values\r\n",
    "\r\n",
    "    to_return = le.transform(values.ravel()) # convert features to numeric codes\r\n",
    "    to_return = to_return.reshape(values.shape[0], -1) # reshape back to old shape\r\n",
    "\r\n",
    "    return labels, to_return"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def buildClassifiers(clf, X_train, X_test, y_train, y_test):\r\n",
    "    # The fit function trains the model (clf)\r\n",
    "    # X_train is a 2d array of the features: each row represents a datapoint, each column represents a feature\r\n",
    "    # y_train is a 1d array of labels. The nth value of the array is the label for the nth row in X_train\r\n",
    "    clf.fit(X_train, y_train)\r\n",
    "    \r\n",
    "    # generating predictions for unseen data\r\n",
    "    y_pred = clf.predict(X_test)\r\n",
    "\r\n",
    "    # calculate the precision, recall and f1 scores to evaluate the classifiers performance on the test data\r\n",
    "    f1 = f1_score(y_test, y_pred, average=\"micro\")\r\n",
    "    precision = precision_score(y_test, y_pred, average=\"micro\")\r\n",
    "    recall = recall_score(y_test, y_pred, average=\"micro\")\r\n",
    "\r\n",
    "    return f1, precision, recall"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following block is used to see if the models work as they should. Training is used to train the model, which is then validated using the validation set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# all data for training\r\n",
    "orths_train, ctags_train, in_chunks_train = feature_extraction(training_file, 'orth', 'ctag')\r\n",
    "df_train = generate_dataframe(orths_train, ctags_train, in_chunks_train)\r\n",
    "labels_train, trained_vals = labels_and_forFitting(df_train)\r\n",
    "\r\n",
    "# all data for validation\r\n",
    "orths_validate, ctags_validate, in_chunks_validate = feature_extraction(validation_file, 'orth', 'ctag')\r\n",
    "df_validate = generate_dataframe(orths_validate, ctags_validate, in_chunks_validate)\r\n",
    "labels_validate, validated = labels_and_forFitting(df_validate)\r\n",
    "\r\n",
    "df_total = pd.concat([df_train, df_validate], axis=0)\r\n",
    "labels_total = np.asarray(df_total['class'].astype(\"category\").cat.codes.tolist())\r\n",
    "X_vals_total = df_total.drop(columns=['class']).values # modify this line to drop multiple columns (always drop class and one more - like a1, a3)\r\n",
    "\r\n",
    "le.fit(X_vals_total.ravel())\r\n",
    "X_total = le.transform(X_vals_total.ravel())\r\n",
    "X_total = X_total.reshape(X_vals_total.shape[0], -1)\r\n",
    "\r\n",
    "# CONTAINS THE TOTAL TRAINING SIZE (train + validate) TO BE USED AS TRAINING FOR THE FINAL TEST SET\r\n",
    "total_train_size = len(df_train)\r\n",
    "total_validate_size = len(df_validate)\r\n",
    "# total_final_train = len(df_total)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "names = ['Naive_Bayes', 'Decision_Tree']\r\n",
    "classifiers = [GaussianNB(), \r\n",
    "               DecisionTreeClassifier(random_state=0)]\r\n",
    "for name, clf in zip(names, classifiers):\r\n",
    "    print('Now classifying', name)\r\n",
    "    aList, bList, cList = list(), list(), list()\r\n",
    "\r\n",
    "    \r\n",
    "    X_train, X_test = X_total[0:total_train_size], X_total[total_train_size:total_train_size + total_validate_size]\r\n",
    "    Y_train, Y_test = labels_total[0:total_train_size], labels_total[total_train_size:total_train_size + total_validate_size]\r\n",
    "\r\n",
    "    f1, precision, recall = buildClassifiers(clf, X_train, X_test, Y_train, Y_test)\r\n",
    "    aList.append(f1)\r\n",
    "    bList.append(precision)\r\n",
    "    cList.append(recall)\r\n",
    "\r\n",
    "    print(\"\\tAverage F1 for {}:\\t\\t\".format(name), np.mean(aList))\r\n",
    "    print(\"\\tAverage Precision for {}:\\t\".format(name), np.mean(bList))\r\n",
    "    print(\"\\tAverage Recall for {}:\\t\\t\".format(name), np.mean(cList))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Now classifying Naive_Bayes\n",
      "\tAverage F1 for Naive_Bayes:\t\t 0.40082198503251143\n",
      "\tAverage Precision for Naive_Bayes:\t 0.4008219850325114\n",
      "\tAverage Recall for Naive_Bayes:\t\t 0.4008219850325114\n",
      "Now classifying Decision_Tree\n",
      "\tAverage F1 for Decision_Tree:\t\t 0.8246636404531141\n",
      "\tAverage Precision for Decision_Tree:\t 0.8246636404531141\n",
      "\tAverage Recall for Decision_Tree:\t\t 0.8246636404531141\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.12 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "009c6260f8c949f949e8d5c80e91b7d11da42068722b763148e110eb4d6efd9c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}